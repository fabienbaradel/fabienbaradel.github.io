<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Fabien Baradel</title>

    <meta name="author" content="Fabien Baradel">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon"> -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><rect x='5' y='5' width='90' height='90' rx='20' ry='20' fill='%23ADD8E6'/><text x='50%25' y='50%25' fill='%23000000' font-size='50' font-family='Arial, sans-serif' font-weight='bold' text-anchor='middle' dominant-baseline='central'>FB</text></svg>" type="image/svg+xml">


    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Fabien Baradel
                </p>
                <p>I am a research scientist at <a href="https://europe.naverlabs.com/">NAVER LABS Europe</a> in Grenoble, France, leading a project on human-centric 3D vision.
                    <br>
                    <br>
                    I completed my PhD at <a href="https://www.insa-lyon.fr">INSA Lyon</a>,
                    advised by <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a> and <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>.
                    During my PhD, I also conducted research at Google, Simon Fraser University, and the University of Guelph.
                    I hold an Engineer's degree (MSc) from <a href="http://www.ensai.fr">ENSAI</a>.
                </p>
                <p style="text-align:center">
                    <a href="mailto:fabien<dot>baradel<at>naverlabs<dot>com">Email</a> &nbsp/&nbsp
                        <a href="./docs/resume/resume_FabienBaradel.pdf">CV</a> &nbsp/&nbsp
                        <a href="https://scholar.google.fr/citations?user=egECWaEAAAAJ&hl=en">Scholar</a>
                        &nbsp/&nbsp
                        <a href="https://github.com/fabienbaradel">Github</a> &nbsp/&nbsp
                        <a href="http://www.linkedin.com/in/fabienbaradel/"> LinkedIn </a> &nbsp/&nbsp
                        <a href="https://twitter.com/fabienbaradel">Twitter</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/oval-bio-photo.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/oval-bio-photo.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                    My research focuses on computer vision and deep learning, with an emphasis on human-centric 3D vision. I work on advancing methods to understand humans from visual data.
                  <!-- Some papers are <span class="highlight">highlighted</span>. -->
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            

        
          <tbody>

            <tr>
                <td style="padding:16px;width:30%;vertical-align:middle"><img src='images/condimen.png' width=100% alt="blind-date"></td>
                <td style="padding:8px;width:70%;vertical-align:middle">
                    <p>
                        <a href="https://arxiv.org/abs/2412.13058">
                            <span class="papertitle">
                                CondiMen: Conditional Multi-Person Mesh Recovery
                            </span>
                        </a>
                        <br>
                        <a href="./">Romain Br&eacutegier</a>,
                        <strong>Fabien Baradel</strong>,
                        <a href="./">Thomas Lucas</a>,
                        <a href="./">Salma Galaaoui</a>,
                        <br>
                        <a href="./">Matthieu Armando</a>,
                        <a href="./">Philippe Weinzaepfel</a>,
                        <a href="./">Gr&eacutegory Rogez</a>,
                        <br>
                        <em>arXiv</em>, 2024 &nbsp
                        <br>
                        <a href="https://arxiv.org/pdf/2402.14654.pdf">PDF</a>
                        /
                        <a href="https://arxiv.org/abs/2402.14654">arXiv</a>
                    </p>
                    <p>
                        A Bayesian method for multi-person human mesh recovery, modeling ambiguities in 3D pose and shape while enabling uncertainty handling and multi-view integration.
                    </p>
                </td>
            </tr>
            

                    <tr>
                        <td style="padding:16px;width:30%;vertical-align:middle"><img src='images/multihmr.gif' width=100% alt="blind-date"></td>
                        <td style="padding:8px;width:70%;vertical-align:middle">
                            <p>
                                <a href="https://arxiv.org/abs/2402.14654">
                                    <span class="papertitle">Multi-HMR: Multi-Person Whole-Body Human Mesh Recovery in a Single Shot
                                    </span>
                                </a>
                                <br>
                                <strong>Fabien Baradel*</strong>,
                                <a href="./">Matthieu Armando</a>,
                                <a href="./">Salma Galaaoui</a>,
                                <a href="./">Romain Br&eacutegier</a>,
                                <br>
                                <a href="./">Philippe Weinzaepfel</a>,
                                <a href="./">Gr&eacutegory Rogez</a>,
                                <a href="./">Thomas Lucas*</a>
                                <br>
                                <em>ECCV</em>, 2024 &nbsp
                                <br>
                                <a href="https://arxiv.org/pdf/2402.14654.pdf">PDF</a>
                                /
                                <a href="https://arxiv.org/abs/2402.14654">arXiv</a>
                                /
                                <a href="https://github.com/naver/multi-hmr">code</a>
                                /
                                <a href="https://huggingface.co/spaces/naver/multi-hmr">demo</a>
                                /
                                <a href="https://rhobin-challenge.github.io/">winner of ROBIN challenge @CVPR'24</a>
                            </p>
                            <p>
                                <!-- A simple yet effective model for multi-person whole-body 3d human pose estimation running in real-time on a GPU and reaching SotA results. -->
                                A simple single-shot model for multi-person 3D human mesh recovery from a single RGB image, leveraging a vision transformer.
                            </p>
                        </td>
                    </tr>
                    

                    <tr>
                        <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/crocoman.png" alt="blind-date"
                                             width=100%
                                             height="125"></td>
                        <td style="padding:8px;width:70%;vertical-align:middle">

                            <p>
                                <a href="https://arxiv.org/pdf/2309.10748.pdf">
                                    <span class="papertitle">Cross-view and Cross-pose Completion for 3D Human Understanding
                                    </span>
                                </a>
                                <br>
                                <a href="./">Matthieu Armando</a>,
                                <a href="./">Salma Galaaoui</a>,
                                <strong>Fabien Baradel</strong>,
                                <a href="./">Thomas Lucas</a>,
                                <a href="./">Vincent Leroy</a>,
                                <br>
                                <a href="./">Romain Br&eacutegier</a>,
                                <a href="./">Philippe Weinzaepfel</a>,
                                <a href="./">Gr&eacutegory Rogez</a>
                                <br>
                                <em>CVPR</em>, 2024 &nbsp
                                <br>
                                <a href="https://arxiv.org/pdf/2311.09104.pdf">PDF</a>
                                /
                                <a href="https://arxiv.org/abs/2311.09104">arXiv</a>
                                /
                                <a href="./">bibtex</a>
                            </p>
                            <p>
                                A self-supervised pre-training strategy for human-centric 3D vision.
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/purposer.png" alt="blind-date" width=100%></td>
                        <td style="padding:8px;width:70%;vertical-align:middle">

                            <p>
                                <a href="https://arxiv.org/pdf/2309.10748.pdf">
                                    <span class="papertitle">Purposer: Putting Human Motion Generation in Context
                                    </span>
                                </a>
                                <br>
                                <a href="./">Nicolas Ugrinovic</a>,
                                <a href="./">Thomas Lucas</a>,
                                <strong>Fabien Baradel</strong>,
                                <a href="./">Philippe Weinzaepfel</a>,
                                <br>
                                <a href="./">Gr&eacutegory Rogez</a>
                                <a href="./">Francesc Moreno-Noguer</a>
                                <br>
                                <em>3DV</em>, 2024 &nbsp
                                <br>
                                <a href="https://arxiv.org/pdf/2404.12942">PDF</a>
                                /
                                <a href="https://arxiv.org/abs/2404.12942">arXiv</a>
                                /
                                <a href="./">bibtex</a>
                            </p>
                            <p>
                                A method able to generate realistic-looking motions that interact with virtual scenes.
                            </p>
                        </td>
                    </tr>


                    <tr>
                        <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/showme.png" alt="blind-date"
                            width=100%></td>
                        <td style="padding:8px;width:70%;vertical-align:middle">

                            <p>
                                <a href="https://arxiv.org/pdf/2309.10748.pdf">
                                    <span class="papertitle">SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction
                                    </span>
                                </a>
                                <br>
                                <a href="./">Anilkumar Swamy</a>,
                                <a href="./">Vincent Leroy</a>,
                                <a href="./">Philippe Weinzaepfel</a>,
                                <strong>Fabien Baradel</strong>,
                                <a href="./">Salma Galaaoui</a>,
                                <a href="./">Romain Br&eacutegier</a>,
                                <a href="./">Matthieu Armando</a>,
                                <a href="./">Jean-Sebastien Franco</a>,
                                <a href="./">Gr&eacutegory Rogez</a>
                                <br>
                                <em>ACVR workshop ICCV</em>, 2023 &nbsp
                                <br>
                                <a href="https://arxiv.org/pdf/2309.10748.pdf">PDF</a>
                                /
                                <a href="https://europe.naverlabs.com/research/showme/">project page</a>
                                /
                                <a href="https://arxiv.org/abs/2309.10748">arXiv</a>
                                /
                                <a href="./bib/showme_ICCV_23.txt">bibtex</a>
                            </p>
                            <p>
                                A new high-quality textured meshes dataset of hand holding an object.
                            </p>
                        </td>
                    </tr>
                    
                    <tr>
                        <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/posegpt.png" alt="blind-date"
                                             width=100%></td>
                        <td style="padding:8px;width:70%;vertical-align:middle">
    
                            <p>
                                <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660409.pdf">
                                    <span class="papertitle">PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting                                      
                                    </span>
                                </a>
                                <br>
                                <a href="./">Thomas Lucas*</a>,
                                <strong>Fabien Baradel*</strong>,
                                <a href="./">Philippe Weinzaepfel</a>,
                                <a href="./">Gr&eacutegory Rogez</a>
                                <br>
                                <em>ECCV</em>, 2022 &nbsp
                                <br>
                                <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660409.pdf">PDF</a>
                                /
                                <a href="https://github.com/naver/PoseGPT">code</a>
                                /
                                <a href="./">arXiv</a>
                                /
                                <a href="./bib/PoseGPT_ECCV_22.txt">bibtex</a>
                            </p>
                            <p>
                                PoseGPT generates a human motion, conditioned on an action label, a duration and optionally on an observed pas human motion.
                                We learn to quantize the human motion into a discrete latent space and we train a GPT-like model to sequentially predicts next discrete latent indices.
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/posebert.png" alt="blind-date"
                                             width=100%></td>
                        <td style="padding:8px;width:70%;vertical-align:middle">
    
                            <p>
                                <a href="https://arxiv.org/abs/2208.10211">
                                    <span class="papertitle">PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling                                        
                                    </span>
                                </a>
                                <br>
                                <strong>Fabien Baradel</strong>,
                                <a href="./">Romain Br&eacutegier</a>,
                                <a href="./">Thibault Groueix</a>,
                                <br>
                                <a href="./">Philippe Weinzaepfel</a>,
                                <a href="./">Yannis Kalantidis</a>,
                                <a href="./">Gr&eacutegory Rogez</a>
                                <br>
                                <em>TPAMI</em>, 2022 &nbsp
                                <br>
                                <a href="https://arxiv.org/abs/2208.10211">arXiv</a>
                                /
                                <a href="./bib/BaradelPoseBERT_arxiv_22.txt">bibtex</a>
                            </p>
                            <p>
                                We propose a generic transformer model for temporal modeling of human and hand shape.
                                We apply this model to different tasks such as pose estimation and future pose prediction.
                                PoseBERT is able to denoise and interpolate which is very important for deploying pose estimation on he wild.
                            </p>
                        </td>
                    </tr>

                    <tr>
                        <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/filtered_cophy.png" alt="blind-date"
                                             width=100%></td>
                        <td style="padding:8px;width:70%;vertical-align:middle">
    
                            <p>
                                <a href="https://openreview.net/forum?id=1L0C5ROtFp">
                                    <span class="papertitle">Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space
                                    </span>
                                </a>
                                <br>
                                <a href="./">Steeven Janny</a>,
                                <strong>Fabien Baradel</strong>,
                                <a href="https://nneverova.github.io/">Natalia Neverova</a>,
                                <br>
                                <a href="./">Madiha Nadri</a>,
                                <a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>,
                                <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                                <br>
                                <em>ICLR</em>, 2022 &nbsp; <strong></strong>
                                <br>
                                <a href="https://openreview.net/pdf?id=1L0C5ROtFp">PDF</a>
                                /
                                <a href="https://openreview.net/forum?id=1L0C5ROtFp">OpenReview</a>
                                /
                                <a href="https://filteredcophy.github.io/">Project Page</a>
                            </p>
                            <p> We propose a model learned in a unsupervised manner which is able to perform counterfactual predictions in pixel space.
                            </p>
                        </td>
                    </tr>

                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/posebert.png" alt="blind-date"
                                         width=100%></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="https://arxiv.org/abs/2110.09243">
                                <span class="papertitle">Leveraging MoCap Data for Human Mesh Recovery
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel*</strong>,
                            <a href="./">Thibault Groueix*</a>,
                            <a href="./">Philippe Weinzaepfel</a>,
                            <br>
                            <a href="./">Romain Br&eacutegier</a>,
                            <a href="./">Yannis Kalantidis</a>,
                            <a href="./">Gr&eacutegory Rogez</a>
                            <br>
                            <em>3DV</em>, 2021 &nbsp
                            <br>
                            <a href="https://arxiv.org/pdf/2110.09243.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/2110.09243">arXiv</a>
                            /
                            <a href="https://slideslive.com/38972169/leveraging-mocap-data-for-human-mesh-recovery?ref=account-96722-presentations">Video-short</a>
                            /
                            <a href="https://slideslive.com/38972310/leveraging-mocap-data-for-human-mesh-recovery?ref=account-96722-presentations">Video-long</a>
                            /
                            <a href="./bib/BaradelPoseBERT_3DV_21.txt">bibtex</a>
                        </p>
                        <p>
                            We show that Mocap data can be used for improving image-based and video-based human mesh recovery methods.
                            We propose a video-based transformer model called PoseBERT which is trained on synthetic data only.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/cf.png" alt="blind-date"
                                         width=100%></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="https://arxiv.org/abs/1909.12000">
                                <span class="papertitle">CoPhy: Counterfactual Learning of Physical Dynamics
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="https://nneverova.github.io/">Natalia Neverova</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
                            <a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>
                            <br>
                            <em>ICLR</em>, 2020 &nbsp; <strong>(Spotlight
                            presentation)</strong>
                            <!--<em>International Conference on Learning Representations-->
                            <!--(<strong>ICLR</strong>)</em>, 2020 <strong>(spotlight)</strong>-->
                            <br>
                            <a href="https://arxiv.org/pdf/1909.12000.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1909.12000">arXiv</a>
                            /
                            <a href="https://github.com/fabienbaradel/cophy">Code-Dataset</a>
                            /
                            <a href="https://youtu.be/95nqaDV9cYM">Video</a>
                            /
                            <a href="./bib/BaradelCophy_ICLR_20.txt">bibtex</a>
                        </p>
                        <p> We introduce a new problem of counterfactual learning of object mechanics from visual input
                            and a benchmark called CoPhy.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/cbt.png" alt="blind-date"
                                         width=100%></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="https://arxiv.org/abs/1906.05743">
                                <span class="papertitle">Learning Video Representations using Contrastive Bidirectional Transformer
                                </span>
                            </a>
                            <br>
                            <a href="http://chensun.me/">Chen Sun</a>,
                            <strong>Fabien Baradel</strong>,
                            <a href="https://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a>,
                            <a href="https://thoth.inrialpes.fr/~schmid/">Cordelia Schmid</a>
                            <br>
                            <em>arXiv preprint</em>, 2019
                            <!--<em>arXiv</em>, 2019-->
                            <br>
                            <a href="https://arxiv.org/pdf/1906.05743.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1906.05743">arXiv</a>
                            /
                            <a href="./bib/SunBaradelMurphySchmid_arxiv_19.txt">bibtex</a>
                        </p>
                        <p> Self-supervised video representation by leveraging ASR and long videos via noise contrastive
                            estimation.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/arxiv18_objectLevelVisualReasoning.png" alt="blind-date"
                                         width=100%></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="https://arxiv.org/abs/1806.06157">
                                <span class="papertitle">Object Level Visual Reasoning in Videos
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="https://nneverova.github.io/">Natalia Neverova</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
                            <a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>
                            <br>
                            <em>ECCV</em>, 2018
                            <!--<em>The IEEE European Conference in Computer Vision-->
                            <!--(<strong>ECCV</strong>)</em>, 2018-->
                            <br>
                            <a href="./eccv18_object_level_visual_reasoning">Project page</a>
                            /
                            <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fabien_Baradel_Object_Level_Visual_ECCV_2018_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1806.06157">arXiv</a>
                            /
                            <a href="./">video</a>
                            /
                            <a href="./bib/BaradelNeverovaWolfMilleMori_objectLevelVisualReasoning_ECCV_18.txt">bibtex</a>
                            /
                            <a href="https://github.com/fabienbaradel/object_level_visual_reasoning">Code</a>
                            /
                            <a href="./masks_data">Complementary Mask Data</a>
                            /
                            <a href="./docs/poster/ECCV2018.pdf">Poster</a>
                        </p>
                        <p> A model capable of learning to reason about semantically meaningful spatio-temporal
                            interactions in videos.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/bmvc18_hands.png" alt="blind-date" width=100% ></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">
                        <p>
                            <a href="./">
                                <span class="papertitle"> Human Activity Recognition with Pose-driven Attention to RGB
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>BMVC</em>, 2018
                            <!--<em>The British Machine Vision Conference (<strong>BMVC</strong>)</em>, 2018-->
                            <br>
                            <a href="./papers/BMVC_18.pdf">PDF</a>
                            /
                            <a href="./bib/BaradelWolfMille_BMVC_18.txt">bibtex</a>
                            /
                            <a href="./docs/poster/BMVC2018.pdf">Poster</a>
                        </p>
                        <p> Human activity recogntion using skeleton data and RGB. We propose a network able to focus on
                            relevant parts of the RGB stream given deep features extracted from the pose stream.
                        </p>
                    </td>
                </tr>


                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/cvpr18_glimpseclouds.png" alt="blind-date" width=100%></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.pdf">
                                <span class="papertitle">Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>,
                            <a href="http://www.uoguelph.ca/~gwtaylor/">Graham Taylor</a>
                            <br>
                            <em>CVPR</em>, 2018
                            <!--<em>The IEEE Conference on Computer Vision and Pattern Recognition-->
                            <!--(<strong>CVPR</strong>)</em>, 2018-->
                            <br>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1802.07898">arXiv</a>
                            /
                            <a href="./cvpr18_glimpseclouds">project page</a>
                            /
                            <a href="https://youtu.be/7yPDYYhaYI4">video</a>
                            /
                            <a href="./bib/BaradelWolfMilleTaylor_CVPR_GlimpseClouds_18.txt">bibtex</a>
                            /
                            <a href="https://www.rsipvision.com/CVPR2018-Tuesday/14/">CVPR Daily</a>
                            /
                            <a href="https://github.com/fabienbaradel/glimpse_clouds">Code</a>
                            /
                            <a href="./docs/poster/CVPR2018.pdf">Poster</a>
                        </p>
                        <p> We propose a new method for human action recognition relying on RGB data only.
                            A visual attention module is able to extract glimpses within each frame.
                            Resulting local descriptors are soft-assigned to distributed workers which are finally
                            classifying the video.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/iccv17_hands.png" alt="blind-date" width=100%></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w11/Baradel_Human_Action_Recognition_ICCV_2017_paper.pdf">
                                <span class="papertitle">Human Action Recognition: Pose-based Attention draws focus to Hands
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>ICCV, Workshop "Hands in Action"</em>, 2017
                            <!--<em>The IEEE International Conference on Computer Vision (<strong>ICCV</strong>), Workshop-->
                            <!--"Hands in Action"</em>, 2017-->
                            <br>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w11/Baradel_Human_Action_Recognition_ICCV_2017_paper.pdf">PDF</a>
                            /
                            <a href="./bib/BaradelWolfMille_ICCV_Hands_17.txt">bibtex</a>
                            /
                            <a href="./docs/poster/ICCVW2017.pdf">Poster</a>
                        </p>
                        <p>A new spatio-temporal attention based mechanism for human action recognition able to
                            automatically attend to most important human hands and detect the most discriminative
                            moments in an action.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/iccv17_taskCV.png" alt="blind-date" width=100%></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w38/Csurka_Discrepancy-Based_Networks_for_ICCV_2017_paper.pdf">
                                <span class="papertitle">Discrepancy-based networks for unsupervised domain adaptation: a comparative
                                    study
                                </span>
                            </a>
                            <br>
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Gabriela-Csurka">Gabriela
                                Csurka</a>,
                            <strong>Fabien Baradel</strong>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Boris-Chidlovskii">Boris
                                Chidlovskii</a>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Stephane-Clinchant">Stephane
                                Clinchant</a>,
                            <br>
                            <em>ICCV, Workshop "Task-CV"</em>, 2017
                            <!--<em>The IEEE International Conference on Computer Vision (<strong>ICCV</strong>), Workshop-->
                            <!--"Task-CV"</em>, 2017-->
                            <br>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w38/Csurka_Discrepancy-Based_Networks_for_ICCV_2017_paper.pdf">PDF</a>
                            /
                            <a href="./bib/CsurkaBaradelChidlovskiiClinchant_ICCV_TaskCV_17.txt">bibtex</a>
                        </p>
                        <p>We introduce a new dataset for Domain Adaptation and show a comparaison between shallow and
                            deep methods based on Maximum Mean Discrepancy.
                        </p>
                    </td>
                </tr>


                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/attention_ntu.gif" alt="blind-date" width="100%"></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="https://arxiv.org/pdf/1703.10106.pdf">
                                <span class="papertitle">Pose-conditioned Spatio-Temporal Attention for Human Action Recognition
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>arXiv preprint</em>, 2017
                            <br>
                            <a href="https://arxiv.org/abs/1703.10106">arXiv</a>
                            /
                            <a href="https://arxiv.org/pdf/1703.10106.pdf">PDF</a>
                            /
                            <a href="./pose_rgb_attention_human_action">project page</a>
                            /
                            <a href="https://youtu.be/kGLGgH_VAZw">video</a>
                            /
                            <a href="./bib/BaradelWolfMille_PoseAttention_arXiv_17.txt">bibtex</a>
                        </p>
                        <p>We introduce an attention-based mechanism around hands on RGB videos conditioned on features
                            extracted from human 3D
                            pose.
                        </p>
                    </td>
                </tr>

                </tbody>

                <td>
                    <heading>PhD Thesis</heading>
                </td>

                <tr>
                    <td style="padding:16px;width:30%;vertical-align:middle"><img src="images/phd_thesis.png" alt="blind-date" width=100% ></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="./docs/phd/manuscript.pdf">
                                <span class="papertitle">Structured Deep Learning for Video Analysis
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>
                            <br>
                            <em>Universit&eacute; de Lyon - INSA Lyon</em>, 2020
                            <br>
                            <strong> Runner-up thesis prize - <a href="http://www.afrif.asso.fr/?p=737">AFRIF</a>
                            </strong>
                            <br>
                            <a href="./docs/phd/manuscript.pdf">PDF</a>
                            /
                            <a href="https://youtu.be/wOmUrfa6XiE">video</a>
                            /
                            <a href="./docs/phd/presentation.pdf">slides-pdf</a>
                            /
                            <a href="./docs/phd/presentation.pptx">slides-pptx</a>
                            /
                            <a href="./bib/Baradel_phdthesis_2020.txt">bibtex</a>
                        </p>
                    </td>
                </tr>

                <!-- <td>
                    <heading>Patents</heading>
                </td>

                <tr>
                    <td width="25%"><img src="images/cmmd_da.png" alt="blind-date" width=100% height="120"></td>
                    <td style="padding:8px;width:70%;vertical-align:middle">

                        <p>
                            <a href="https://patents.google.com/patent/US20180253627A1/en">
                                <span class="papertitle">Conditional adaptation network for image classification
                                </span>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Gabriela-Csurka">Gabriela
                                Csurka</a>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Boris-Chidlovskii">Boris
                                Chidlovskii</a>,
                            <br>
                            <em>US Patent App. 15/450,620 - Xerox Corp</em>, 2017
                            <br>
                            <a href="https://patentimages.storage.googleapis.com/0e/64/38/504605db411662/US20180253627A1.pdf">PDF</a>
                            /
                            <a href="./bib/BaradelCsurkaChidlovskii_USPatent_17.txt">bibtex</a>
                        </p>
                        <p>We introduce a new method based on Conditional Maximum Mean Discrepancy for domain adaptation
                            on image classification.
                        </p>
                    </td>
                </tr> -->


                <tbody>
            </table>

            <!--Reviewing activity-->
            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Reviewer</heading>
                        ICCV 2021, ECCV 2020, CVPR 2019-2020, ICML 2019, NIPS 2018, IJCV, TPAMI
                    </td>
                </tr>
            </table> -->

            <!-- ######## TEACHING #########-->
<!-- 

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="100%" valign="middle">
                        <heading id="news">Talks</heading>
                        <ul>
                            <li>
                                <a href="./">Counterfactual learning for physical dynamics</a> - ICML 2020 workshop
                                "Object-Oriented Perception, Reasoning, and Causality" - July 2020
                            </li>
                            <li>
                                <a href="https://drive.google.com/file/d/1z6srYQP7VBwN8gDo4RT-1mAPWG0QFxOq/view?usp=sharing">Visual
                                    Reasoning in Videos</a> - GDR ISIS-IA "Reasoning from Visual Signal" - October 2018
                            </li>
                            <li>
                                <a href="https://drive.google.com/file/d/15QRxTqNfPfWex5wyrkjI0CdrDLH1xtlN/view?usp=sharing">Pose
                                    and Attention for Action Recognition</a> - GDR ISIS "Pose and Gesture" - December
                                2017
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table> -->


            <!-- ######## TEACHING #########-->


            <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <heading>Teaching</heading>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
                <tr>
                    <td width="25%"><img src="./images/teaching.png" alt="teaching" width="160" height="160"></td>
                    <td width="75%" valign="center">
                        <p>
                            <a href="./">
                                <span class="papertitle">Machine Learning</span>
                            </a>
                            <br>
                            <em>Science U - M2 Info - 28h30 (CM+TP) - 2018/2019</em>
                            <br>
                            <a href="https://github.com/fabienbaradel/intro_ml">Github repo</a>
                            / Slides:
                            <a href="./docs/teaching/intro_ml/cours_1.pdf">1</a>
                            -
                            <a href="./docs/teaching/intro_ml/cours_2.pdf">2</a>
                            / Exercises:
                            <a href="./docs/teaching/intro_ml/tp_1.pdf">1</a>
                            -
                            <a href="./docs/teaching/intro_ml/tp_2.pdf">2</a>
                        </p>
                        <p>
                            <a href="./">
                                <span class="papertitle">Regression Modelling</span>
                            </a>
                            <br>
                            <em>Univ Lyon 1 - M2 Data Science - 12h (TP) - Fall 2017</em>
                        </p>
                        <p>
                            <a href="./">
                                <span class="papertitle">Probability & Statistics</span>
                            </a>
                            <br>
                            <em>Univ Lyon 1 - L2 Info & Maths-Eco - 12h+8h (TP) - Fall 2017</em>
                        </p>
                        <p>
                            <a href="./">
                                <span class="papertitle">Mathematics</span>
                            </a>
                            <br>
                            <em>EPITA - 1st year - 24h (CM+TD) - September 2017</em>
                        </p>
                        <p>
                            <a href="./images/tensorflow_ensai_SID_13_01_17.pdf">
                                <span class="papertitle">Introduction to Deep Learning with Tensorflow</span>
                            </a>
                            <br>
                            <em>ENSAI - MSc Data Science - 6h - January 2017</em>
                            <br>
                            <a href="https://github.com/fabienbaradel/Tensorflow-tutorials/">Github repo</a>
                            /
                            <a href="./docs/tensorflow_ensai_SID_13_01_17.pdf">slides</a>
                        </p>

                    </td>
                </tr>

            </table> -->


            <!-- ######## THANKS #########-->


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <br>
                        <p align="right">
                            <font size="2">
                                <a href="https://jonbarron.info">Awesome webpage...</a>
                            </font>
                        </p>
                    </td>
                </tr>
            </table>
            <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));















































































            </script>
            <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}














































































            </script>
        </td>
    </tr>
</table>
</body>
</html>

