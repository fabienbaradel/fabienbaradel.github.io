<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
    <meta name=viewport content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    .new {
    background-color : #cc0000; color:white; border-radius : 5px; padding :1px; font-size : 14px; margin : 0 5px;}



























    </style>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <title>Fabien Baradel</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-86789415-1', 'auto');
  ga('send', 'pageview');




















    </script>
</head>
<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle">
                        <p align="center">
                            <name>Fabien Baradel</name>
                        </p>

                        <!-- ######## ABSTRACT #########-->

                        <p>I am a third year PhD Candidate at <a href="https://www.insa-lyon.fr">INSA Lyon</a> - <a
                                href="https://liris.cnrs.fr/?set_language=fr">LIRIS</a>.
                            <br>
                            I am working on <strong>Machine Learning</strong> and <strong>Computer Vision</strong>.
                            <br>
                            My supervisors are <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a> and <a
                                    href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>.
                            <br>
                            My PhD focuses on video understanding.
                        </p>

                        <p align="center">

                            PhD thesis title:
                            <br>
                            <strong>"Deep Learning for Human Understanding:
                                <br> poses, gestures, activities"</strong>
                            <br>funded by the ANR/NSREC DeepVision project.
                        </p>
                        <p>I received my Engineer's degree (MSc) from <a href="http://www.ensai.fr">ENSAI</a> with a
                            major in Big Data - Data Science.
                            Previously I've been intern at <a href="http://www.europe.naverlabs.com/">Xerox Research
                                Centre
                                Europe</a>.

                        </p>


                        <!-- ######## INFO  #########-->

                        <p align=center>
                            <a href="mailto:fabien<dot>baradel<at>liris<dot>cnrs<dot>fr">Email</a> &nbsp/&nbsp
                            <a href="./docs/resume/academic_resume.pdf">CV</a> &nbsp/&nbsp
                            <a href="https://github.com/fabienbaradel">Github</a> &nbsp/&nbsp
                            <a href="http://www.linkedin.com/in/fabienbaradel/"> LinkedIn </a> &nbsp/&nbsp
                            <a href="https://twitter.com/fabienbaradel">Twitter</a>
                        </p>
                    </td>
                    <td width="33%">
                        <img src="./images/oval-bio-photo.png">
                    </td>
                </tr>
            </table>

            <!-- ######## NEWS  #########-->

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="100%" valign="middle">
                        <heading id="news">News</heading>
                        <ul>
                            <li>July 2018: One paper accepted to BMVC'18 and one paper accepted to ECCV'18 !
                            </li>
                            <li>June 2018: New paper on arXiv - <a
                                    href="https://arxiv.org/abs/1806.06157">"Object level visual reasoning in
                                videos"</a>
                            </li>
                            <li>February 2018: One paper accepted to CVPR'18 ! More news coming soon.
                            </li>
                            <li>February 2018: I will spend 4 months at <a
                                    href="http://vml.cs.sfu.ca">Vision & Media lab</a> in Simon Fraser University under
                                direction of <a
                                        href="http://www.cs.sfu.ca/~mori/">Dr. Greg Mori</a>
                            </li>
                            <li>August 2017: Two papers accepted to ICCVW'17 (<a
                                    href="http://adas.cvc.uab.es/task-cv2017/">TASK-CV</a> - <a
                                    href="http://icvl.ee.ic.ac.uk/hands17/">HANDS</a>)
                            </li>
                            <li>Summer 2017: I spend the summer at University of Guelph under direction of <a
                                    href="http://www.uoguelph.ca/~gwtaylor/">Dr. Graham Taylor</a>
                            </li>
                            <li>June 2017: I am participating at the <a
                                    href="https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/">MILA
                                Deep & Reinforcement
                                Learning Summer School</a> in Montreal!
                            </li>
                            <li>March 2017: New arXiv paper! <a href="./pose_rgb_attention_human_action"><B>
                                "Pose-conditioned Spatio-Temporal Attention for Human Action
                                Recognition" </B></a> </span>
                            </li>
                            <li>January 2017: Seminar on "Introduction to Deep Learning with Tensorflow" at ENSAI
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>


            <!-- ######## RESEARCH INTERESTS  #########-->

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Research</heading>
                        <p>
                            My main researchs focus on video understanding.
                            I am interested in machine learning, deep learning, computer vision and domain adaptation.
                            I am also interested about causal inference and unsupervised learning in general.
                        </p>
                    </td>
                </tr>
            </table>


            <!-- ######## PAPERS #########-->


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                <tbody>

                <tr>
                    <td width="25%"><img src="images/arxiv18_objectLevelVisualReasoning.png" alt="blind-date"
                                         width="200"
                                         height="155"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/abs/1806.06157">
                                <papertitle>Object Level Visual Reasoning in Videos
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="https://nneverova.github.io/">Natalia Neverova</a>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <a href="http://www.cs.sfu.ca/~mori/">Greg Mori</a>
                            <br>
                            <em>The IEEE European Conference in Computer Vision
                                (<strong>ECCV</strong>)</em>, 2018
                            <br>
                            <a href="./eccv18_object_level_visual_reasoning">Project page</a>
                            /
                            <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Fabien_Baradel_Object_Level_Visual_ECCV_2018_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1806.06157">arXiv</a>
                            /
                            <a href="./">video</a>
                            /
                            <a href="./bib/BaradelNeverovaWolfMilleMori_objectLevelVisualReasoning_ECCV_18.txt">bibtex</a>
                            /
                            <a href="https://github.com/fabienbaradel/object_level_visual_reasoning">Code</a>
                            /
                            <a href="./masks_data">Complementary Mask Data</a>
                        </p>
                        <p> A model capable of learning to reason about semantically meaningful spatio-temporal
                            interactions in videos.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="images/bmvc18_hands.png" alt="blind-date" width="200" height="100"></td>
                    <td width="75%" valign="top">
                        <p>
                            <a href="./">
                                <papertitle> Human Activity Recognition with Pose-driven Attention to RGB
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>The British Machine Vision Conference (<strong>BMVC</strong>)</em>, 2018
                            <br>
                            <a href="./papers/BMVC_18.pdf">PDF</a>
                            /
                            <a href="./bib/BaradelWolfMille_BMVC_18.txt">bibtex</a>
                        </p>
                        <p> Human activity recogntion using skeleton data and RGB. We propose a network able to focus on
                            relevant parts of the RGB stream given deep features extracted from the pose stream.
                        </p>
                    </td>
                </tr>


                <tr>
                    <td width="25%"><img src="images/cvpr18_glimpseclouds.png" alt="blind-date" width="200"
                                         height="155"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.pdf">
                                <papertitle>Glimpse Clouds: Human Activity Recognition from Unstructured Feature Points
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <a href="http://www.uoguelph.ca/~gwtaylor/">Graham Taylor</a>
                            <br>
                            <em>The IEEE Conference on Computer Vision and Pattern Recognition
                                (<strong>CVPR</strong>)</em>, 2018
                            <br>
                            <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Baradel_Glimpse_Clouds_Human_CVPR_2018_paper.pdf">PDF</a>
                            /
                            <a href="https://arxiv.org/abs/1802.07898">arXiv</a>
                            /
                            <a href="./cvpr18_glimpseclouds">project page</a>
                            /
                            <a href="https://youtu.be/7yPDYYhaYI4">video</a>
                            /
                            <a href="./bib/BaradelWolfMilleTaylor_CVPR_GlimpseClouds_18.txt">bibtex</a>
                            /
                            <a href="https://www.rsipvision.com/CVPR2018-Tuesday/14/">CVPR Daily</a>
                            /
                            <a href="https://github.com/fabienbaradel/glimpse_clouds">Code</a>
                        </p>
                        <p> We propose a new method for human action recognition relying on RGB data only.
                            A visual attention module is able to extract glimpses within each frame.
                            Resulting local descriptors are soft-assigned to distributed workers which are finally
                            classifying the video.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="images/iccv17_hands.png" alt="blind-date" width="200" height="190"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w11/Baradel_Human_Action_Recognition_ICCV_2017_paper.pdf">
                                <papertitle>Human Action Recognition: Pose-based Attention draws focus to Hands
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>The IEEE International Conference on Computer Vision (<strong>ICCV</strong>), Workshop
                                "Hands in Action"</em>, 2017
                            <br>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w11/Baradel_Human_Action_Recognition_ICCV_2017_paper.pdf">PDF</a>
                            /
                            <a href="./bib/BaradelWolfMille_ICCV_Hands_17.txt">bibtex</a>
                        </p>
                        <p>A new spatio-temporal attention based mechanism for human action recognition able to
                            automatically attend to most important human hands and detect the most discriminative
                            moments in an action.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%"><img src="images/iccv17_taskCV.png" alt="blind-date" width="200" height="95"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w38/Csurka_Discrepancy-Based_Networks_for_ICCV_2017_paper.pdf">
                                <papertitle>Discrepancy-based networks for unsupervised domain adaptation: a comparative
                                    study
                                </papertitle>
                            </a>
                            <br>
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Gabriela-Csurka">Gabriela
                                Csurka</a>,
                            <strong>Fabien Baradel</strong>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Boris-Chidlovskii">Boris
                                Chidlovskii</a>,
                            <a href="http://www.europe.naverlabs.com/NAVER-LABS-Europe/People/Stephane-Clinchant">Stephane
                                Clinchant</a>,
                            <br>
                            <em>The IEEE International Conference on Computer Vision (<strong>ICCV</strong>), Workshop
                                "Task-CV"</em>, 2017
                            <br>
                            <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w38/Csurka_Discrepancy-Based_Networks_for_ICCV_2017_paper.pdf">PDF</a>
                            /
                            <a href="./bib/CsurkaBaradelChidlovskiiClinchant_ICCV_TaskCV_17.txt">bibtex</a>
                        </p>
                        <p>We introduce a new dataset for Domain Adaptation and show a comparaison between shallow and
                            deep methods based on Maximum Mean Discrepancy.
                        </p>
                    </td>
                </tr>


                <tr>
                    <td width="25%"><img src="images/attention_ntu.gif" alt="blind-date" width="160" height="160"></td>
                    <td width="75%" valign="top">

                        <p>
                            <a href="https://arxiv.org/pdf/1703.10106.pdf">
                                <papertitle>Pose-conditioned Spatio-Temporal Attention for Human Action Recognition
                                </papertitle>
                            </a>
                            <br>
                            <strong>Fabien Baradel</strong>,
                            <a href="http://liris.cnrs.fr/christian.wolf/">Christian Wolf</a>,
                            <a href="http://www.rfai.li.univ-tours.fr/PagesPerso/jmille/">Julien Mille</a>
                            <br>
                            <em>arXiv preprint</em>, 2017
                            <br>
                            <a href="https://arxiv.org/abs/1703.10106">arXiv</a>
                            /
                            <a href="https://arxiv.org/pdf/1703.10106.pdf">PDF</a>
                            /
                            <a href="./pose_rgb_attention_human_action">project page</a>
                            /
                            <a href="https://youtu.be/kGLGgH_VAZw">video</a>
                            /
                            <a href="./bib/BaradelWolfMille_PoseAttention_arXiv_17.txt">bibtex</a>
                        </p>
                        <p>We introduce an attention-based mechanism around hands on RGB videos conditioned on features
                            extracted from human 3D
                            pose.
                        </p>
                    </td>
                </tr>

                </tbody>
            </table>


            <!-- ######## TEACHING #########-->


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <heading>Teaching</heading>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
                <tr>
                    <td width="25%"><img src="./images/teaching.png" alt="teaching" width="160" height="160"></td>
                    <td width="75%" valign="center">
                        <p>
                            <a href="./">
                                <papertitle>Regression Modelling</papertitle>
                            </a>
                            <br>
                            <em>Univ Lyon 1 - M2 Data Science - 12h (TP) - Fall 2017</em>
                        </p>
                        <p>
                            <a href="./">
                                <papertitle>Probability & Statistics</papertitle>
                            </a>
                            <br>
                            <em>Univ Lyon 1 - L2 Info & Maths-Eco - 12h+8h (TP) - Fall 2017</em>
                        </p>
                        <p>
                            <a href="./">
                                <papertitle>Mathematics</papertitle>
                            </a>
                            <br>
                            <em>EPITA - 1st year - 24h (CM+TD) - September 2017</em>
                        </p>
                        <p>
                            <a href="./images/tensorflow_ensai_SID_13_01_17.pdf">
                                <papertitle>Introduction to Deep Learning with Tensorflow</papertitle>
                            </a>
                            <br>
                            <em>ENSAI - MSc Data Science - 6h - January 2017</em>
                            <br>
                            <a href="https://github.com/fabienbaradel/Tensorflow-tutorials/">Github repo</a>
                            /
                            <a href="./docs/tensorflow_ensai_SID_13_01_17.pdf">slides</a>
                        </p>

                    </td>
                </tr>

            </table>


            <!-- ######## THANKS #########-->


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <br>
                        <p align="right">
                            <font size="2">
                                <a href="https://jonbarron.info">Awesome webpage...</a>
                            </font>
                        </p>
                    </td>
                </tr>
            </table>
            <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));


















































            </script>
            <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}

















































            </script>
        </td>
    </tr>
</table>
</body>
</html>

